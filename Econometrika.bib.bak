% Encoding: UTF-8

@Article{Yiu2018,
  author    = {Sean Yiu and Li Su},
  title     = {Covariate association eliminating weights: a unified weighting framework for causal effect estimation},
  journal   = {Biometrika},
  year      = {2018},
  month     = {apr},
  doi       = {10.1093/biomet/asy015},
  file      = {:D\:/Github/Informs/Articles/asy015.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Gentry2018,
  author    = {Matthew L. Gentry and Timothy P. Hubbard and Denis Nekipelov and Harry J. Paarsch},
  title     = {Structural Econometrics of Auctions: A Review},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2018},
  volume    = {9},
  number    = {2-4},
  pages     = {79--302},
  doi       = {10.1561/0800000031},
  file      = {:Articles/9781680834475-summary.pdf:PDF},
  publisher = {Now Publishers},
}

@Article{Koop2009,
  author    = {Gary Koop},
  title     = {Bayesian Multivariate Time Series Methods for Empirical Macroeconomics},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2009},
  volume    = {3},
  number    = {4},
  pages     = {267--358},
  abstract  = {Macroeconomic practitioners frequently work with multivariate time series models such as VARs, factor augmented VARs as well as time-varying parameter versions of these models (including variants with multivariate stochastic volatility). These models have a large number of parameters and, thus, over-parameterization problems may arise. Bayesian methods have become increasingly popular as a way of overcoming these problems. In this monograph, we discuss VARs, factor augmented VARs and time-varying parameter extensions and show how Bayesian inference proceeds. Apart from the simplest of VARs, Bayesian inference requires the use of Markov chain Monte Carlo methods developed for state space models and we describe these algorithms. The focus is on the empirical macroeconomist and we offer advice on how to use these models and methods in practice and include empirical illustrations. A website provides Matlab code for carrying out Bayesian inference in these models.},
  doi       = {10.1561/0800000013},
  publisher = {Now Publishers},
}

@Article{Lee2010,
  author    = {Lung-fei Lee},
  title     = {Estimation of Spatial Panels},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2010},
  volume    = {4},
  number    = {1-2},
  pages     = {1--164},
  doi       = {10.1561/0800000015},
  publisher = {Now Publishers},
}

@Article{Lechner2010,
  author    = {Michael Lechner},
  title     = {The Estimation of Causal Effects by Difference-in-Difference {MethodsEstimation} of Spatial Panels},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2010},
  volume    = {4},
  number    = {3},
  pages     = {165--224},
  abstract  = { This survey gives a brief overview of the literature on the difference-in-difference (DiD) estimation strategy and discusses major issues using a treatment effects perspective. In this sense, this survey gives a somewhat different view on DiD than the standard textbook discussion of the DiD model, but it will not be as complete as the latter. It contains some extensions of the literature, for example, a discussion of, and suggestions for nonlinear DiD estimators as well as DiD estimators based on propensity-score type matching methods.},
  doi       = {10.1561/0800000014},
  publisher = {Now Publishers},
}

@Article{Greene2006,
  author    = {William Greene},
  title     = {Functional Form and Heterogeneity in Models for Count Data},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2006},
  volume    = {1},
  number    = {2},
  pages     = {113--218},
  abstract  = { This study presents several extensions of the most familiar models for count data, the Poisson and negative binomial models. We develop an encompassing model for two well-known variants of the negative binomial model (the NB1 and NB2 forms). We then analyze some alternative approaches to the standard log gamma model for introducing heterogeneity into the loglinear conditional means for these models. The lognormal model provides a versatile alternative specification that is more flexible (and more natural) than the log gamma form, and provides a platform for several "two part" extensions, including zero inflation, hurdle, and sample selection models. (We briefly present some alternative approaches to modeling heterogeneity.) We also resolve some features in Hausman, Hall and Griliches (1984, Economic models for count data with an application to the patents–R&D relationship, Econometrica 52 , 909–938) widely used panel data treatments for the Poisson and negative binomial models that appear to conflict with more familiar models of fixed and random effects. Finally, we consider a bivariate Poisson model that is also based on the lognormal heterogeneity model. Two recent applications have used this model. We suggest that the correlation estimated in their model frameworks is an ambiguous measure of the correlation of the variables of interest, and may substantially overstate it. We conclude with a detailed application of the proposed methods using the data employed in one of the two aforementioned bivariate Poisson studies.},
  doi       = {10.1561/0800000008},
  publisher = {Now Publishers},
}

@Article{Startz2014,
  author    = {Richard Startz},
  title     = {Choosing the More Likely Hypothesis},
  journal   = {Foundations and Trends{\textregistered} in Econometrics},
  year      = {2014},
  volume    = {7},
  number    = {2},
  pages     = {119--189},
  abstract  = {Much of economists' statistical work centers on testing hypotheses in which parameter values are partitioned between a null hypothesis and an alternative hypothesis in order to distinguish two views about the world. Our traditional procedures are based on the probabilities of a test statistic under the null but ignore what the statistics say about the probability of the test statistic under the alternative. Traditional procedures are not intended to provide evidence for the relative probabilities of the null versus alternative hypotheses, but are regularly treated as if they do. Unfortunately, when used to distinguish two views of the world, traditional procedures can lead to wildly misleading inference. In order to correctly distinguish between two views of the world, one needs to report the probabilities of the hypotheses given parameter estimates rather than the probability of the parameter estimates given the hypotheses. This monograph shows why failing to consider the alternative hypothesis often leads to incorrect conclusions. I show that for most standard econometric estimators, it is not difficult to compute the proper probabilities using Bayes theorem. Simple formulas that require only information already available in standard estimation reports are provided. I emphasize that frequentist approaches for deciding between the null and alternative hypothesis are not free of priors. Rather, the usual procedures involve an implicit, unstated prior that is likely to be far from scientifically neutral.},
  doi       = {10.1561/0800000028},
  publisher = {Now Publishers},
}

@Article{Keele2018,
  author        = {Luke Keele and Dylan Small},
  title         = {Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference using Five Empirical Applications},
  __markedentry = {[dulunche:6]},
  abstract      = {Matching methods have become one frequently used method for statistical adjustment under a selection on observables identification strategy. Matching methods typically focus on modeling the treatment assignment process rather than the outcome. Many of the recent advances in matching allow for various forms of covariate prioritization. This allows analysts to emphasize the adjustment of some covariates over others, typically based on subject matter expertise. While flexible machine learning methods have a long history of being used for statistical prediction, they have generally seen little use in causal modeliing. However, recent work has developed flexible machine learning methods based on outcome models for the esimation of causal effects. These methods are designed to use little analyst input. All covariate prioritization is done by the learner. In this study, we replicate five published studies that used customized matching methods for covariate prioritization. In each of these studies, subsets of covariates were given priority in the match based on substantive expertise. We replicate these studies using three different machine learning methods that have designed for causal modeling. We find that in almost every case matching and machine learning methods produce identical results. We conclude by discussing the implications for applied analysts.},
  date          = {2018-05-09},
  eprint        = {1805.03743v1},
  eprintclass   = {stat.AP},
  eprinttype    = {arXiv},
  file          = {:Articles/1805.03743[2293].pdf:PDF},
  keywords      = {stat.AP},
}

@Article{Imbens2018,
  author    = {Guido Imbens},
  title     = {Comments on understanding and misunderstanding randomized controlled trials: A commentary on Cartwright and Deaton},
  journal   = {Social Science {\&} Medicine},
  year      = {2018},
  month     = {apr},
  doi       = {10.1016/j.socscimed.2018.04.028},
  file      = {:Articles/1-s2.0-S0277953618301953-main[2291].pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Comment{jabref-meta: databaseType:bibtex;}
