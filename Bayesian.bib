% Encoding: UTF-8

@Article{Webb2018,
  author    = {J. Angus Webb and Siobhan C. de Little and Kimberly A. Miller and Michael J. Stewardson},
  title     = {Quantifying and predicting the benefits of environmental flows: Combining large-scale monitoring data and expert knowledge within hierarchical Bayesian models},
  journal   = {Freshwater Biology},
  year      = {2018},
  month     = {feb},
  doi       = {10.1111/fwb.13069},
  file      = {:C\:/Users/cjdua/Documents/Webb_et_al-2018-Freshwater_Biology.pdf:PDF},
  publisher = {Wiley-Blackwell},
}

@TechReport{li2018integrated,
  author      = {Li, Yong and Yu, Jun and Zeng, Tao},
  title       = {Integrated Deviance Information Criterion for Latent Variable Models},
  institution = {Singapore Management University, School of Economics},
  year        = {2018},
  file        = {:Articles/DIC_LatentVariable25.pdf:PDF},
}

@Article{smith2002data,
  author    = {Smith, George Davey and Ebrahim, Shah},
  title     = {Data dredging, bias, or confounding: They can all get you into the BMJ and the Friday papers},
  journal   = {BMJ: British Medical Journal},
  year      = {2002},
  volume    = {325},
  number    = {7378},
  pages     = {1437},
  publisher = {BMJ Publishing Group},
}

@Article{tipping2004bayesian,
  author    = {Tipping, Michael E},
  title     = {Bayesian inference: An introduction to principles and practice in machine learning},
  journal   = {Lecture notes in computer science},
  year      = {2004},
  volume    = {3176},
  pages     = {41--62},
  publisher = {Springer},
}

@Article{wagenmakers2007practical,
  author    = {Wagenmakers, Eric-Jan},
  title     = {A practical solution to the pervasive problems ofp values},
  journal   = {Psychonomic bulletin \& review},
  year      = {2007},
  volume    = {14},
  number    = {5},
  pages     = {779--804},
  publisher = {Springer},
}

@Book{lynch2007introduction,
  title     = {Introduction to applied Bayesian statistics and estimation for social scientists},
  publisher = {Springer Science \& Business Media},
  year      = {2007},
  author    = {Lynch, Scott M},
}

@Article{scutari2009learning,
  author  = {Scutari, Marco},
  title   = {Learning Bayesian networks with the bnlearn R package},
  journal = {arXiv preprint arXiv:0908.3817},
  year    = {2009},
}

@Article{Friedman2010Regularization,
  author  = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
  title   = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  journal = {Journal of Statistical Software},
  year    = {2010},
  volume  = {33},
  number  = {1},
  pages   = {1--22},
  url     = {http://www.jstatsoft.org/v33/i01/},
}

@InBook{Sebastiani2010,
  chapter   = {10},
  pages     = {175--208},
  title     = {Bayesian Networks},
  publisher = {Springer US},
  year      = {2010},
  author    = {Sebastiani, Paola and Abad, Maria M. and Ramoni, Marco F.},
  editor    = {Maimon, Oded and Rokach, Lior},
  address   = {Boston, MA},
  isbn      = {978-0-387-09823-4},
  abstract  = {Bayesian networks are today one of the most promising approaches to Data Mining and knowledge discovery in databases. This chapter reviews the fundamental aspects of Bayesian networks and some of their technical aspects, with a particular emphasis on the methods to induce Bayesian networks from different types of data. Basic notions are illustrated through the detailed descriptions of two Bayesian network applications: one to survey data and one to marketing data.},
  booktitle = {Data Mining and Knowledge Discovery Handbook},
  doi       = {10.1007/978-0-387-09823-4_10},
  file      = {:D\:/Dropbox/Articles/[9] 2010 Data Mining and Knowledge Discovery Handbook.pdf:PDF},
  url       = {https://doi.org/10.1007/978-0-387-09823-4_10},
}

@Article{Simon2011Coxnet,
  author  = {Noah Simon and Jerome Friedman and Trevor Hastie and Rob Tibshirani},
  title   = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
  journal = {Journal of Statistical Software},
  year    = {2011},
  volume  = {39},
  number  = {5},
  pages   = {1--13},
  url     = {http://www.jstatsoft.org/v39/i05/},
}

@Article{dienes2011bayesian,
  author    = {Dienes, Zoltan},
  title     = {Bayesian versus orthodox statistics: Which side are you on?},
  journal   = {Perspectives on Psychological Science},
  year      = {2011},
  volume    = {6},
  number    = {3},
  pages     = {274--290},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
}

@Misc{etz2016using,
  author    = {Etz, Alexander},
  title     = {Using Bayes Factors to Get the Most Out of Linear Regression: a Practical Guide Using R},
  year      = {2016},
  publisher = {Citeseer},
}

@Article{benjamin2017redefine,
  author  = {Benjamin, Daniel J and Berger, James O and Johannesson, Magnus and Nosek, Brian A and Wagenmakers, Eric-Jan and Berk, Richard and Bollen, Kenneth A and Brembs, Bj{\"o}rn and Brown, Lawrence and Camerer, Colin and others},
  title   = {Redefine statistical significance},
  journal = {Nature Human Behaviour},
  year    = {2017},
}

@Article{lobo2011short,
  author    = {Lobo, Miguel Sousa},
  title     = {A Short JAGS Tutorial with an Interpersonal Perception/Social Network Example},
  year      = {2011},
  file      = {:C\:/Users/cjdua/Documents/JAGS_tutorial_Lobo.pdf:PDF},
  publisher = {Citeseer},
}

@InProceedings{zhang2018fairness,
  author    = {Zhang, Junzhe and Bareinboim, Elias},
  title     = {Fairness in Decision-Making--The Causal Explanation Formula},
  booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence},
  year      = {2018},
  doi       = {10.1155/2018/5176920},
  file      = {:C\:/Users/cjdua/Documents/r30.pdf:PDF},
}

@Article{gelman2017stents,
  author = {Gelman, Andrew and Nallamothu, Brahmajee},
  title  = {Stents: An exploration of design, measurement, analysis, and reporting in clinical research},
  year   = {2017},
  file   = {:C\:/Users/cjdua/Documents/Stent An exploration of design measurement analysis and reporting in.pdf:PDF},
}

@Article{trangucci2018voting,
  author  = {Trangucci, Rob and Ali, Imad and Gelman, Andrew and Rivers, Doug},
  title   = {Voting patterns in 2016: Exploration using multilevel regression and poststratification (MRP) on pre-election polls},
  journal = {arXiv preprint arXiv:1802.00842},
  year    = {2018},
  file    = {:C\:/Users/cjdua/Documents/1802.00842.pdf:PDF},
}

@Article{McShane2017,
  author    = {Blakeley B. McShane and David Gal},
  title     = {Statistical Significance and the Dichotomization of Evidence},
  journal   = {Journal of the American Statistical Association},
  year      = {2017},
  volume    = {112},
  number    = {519},
  pages     = {885--895},
  month     = {jul},
  abstract  = {In light of recent concerns about reproducibility and replicability, the ASA issued a Statement on Statistical Significance and p-values aimed at those who are not primarily statisticians. While the ASA Statement notes that statistical significance and p-values are “commonly misused and misinterpreted,” it does not discuss and document broader implications of these errors for the interpretation of evidence. In this article, we review research on how applied researchers who are not primarily statisticians misuse and misinterpret p-values in practice and how this can lead to errors in the interpretation of evidence. We also present new data showing, perhaps surprisingly, that researchers who are primarily statisticians are also prone to misuse and misinterpret p-values thus resulting in similar errors. In particular, we show that statisticians tend to interpret evidence dichotomously based on whether or not a p-value crosses the conventional 0.05 threshold for statistical significance. We discuss implications and offer recommendations.

KEYWORDS: Null hypothesis significance testing, p-value, Statistical significance, Sociology of science},
  doi       = {10.1080/01621459.2017.1289846},
  file      = {:Articles/Statistical Significance and the Dichotomization of Evidence.pdf:PDF},
  publisher = {Informa {UK} Limited},
}

@Article{Woltman2012,
  author    = {Heather Woltman and Andrea Feldstain and J. Christine MacKay and Meredith Rocchi},
  title     = {An introduction to hierarchical linear modeling},
  journal   = {Tutorials in Quantitative Methods for Psychology},
  year      = {2012},
  volume    = {8},
  number    = {1},
  pages     = {52--69},
  month     = {feb},
  abstract  = {This tutorial aims to introduce Hierarchical Linear Modeling (HLM). A simple explanation of HLM is provided that describes when to use this statistical technique and identifies key factors to consider before conducting this analysis. The first section of the tutorial defines HLM, clarifies its purpose, and states its advantages. The second section explains the mathematical theory, equations, and conditions underlying HLM. HLM hypothesis testing is performed in the third section. Finally, the fourth section provides a practical example of running HLM, with which readers can follow along. Throughout this tutorial, emphasis is placed on providing a straightforward overview of the basic principles of HLM.},
  doi       = {10.20982/tqmp.08.1.p052},
  file      = {:D\:/Github/Informs/Articles/p052.pdf:PDF;:C\:/Users/cjdua/Downloads/p052.zip:zip},
  publisher = {The Quantitative Methods for Psychology},
}

@Article{Song2012,
  author    = {Xin-Yuan Song and Sik-Yum Lee},
  title     = {A tutorial on the Bayesian approach for analyzing structural equation models},
  journal   = {Journal of Mathematical Psychology},
  year      = {2012},
  volume    = {56},
  number    = {3},
  pages     = {135--148},
  month     = {jun},
  abstract  = {In this paper, we provide a tutorial exposition on the Bayesian approach in analyzing structural equation models (SEMs). SEMs, which can be regarded as regression models with observed and latent variables, have been widely applied to substantive research. However, the classical methods and most commercial software in this area are based on the covariance structure approach, which would encounter serious difficulties when dealing with complicated models and/or data structures. In contrast, the Bayesian approach has much more flexibility in handling complex situations. We give a brief introduction to SEMs and a detailed description of how to apply the Bayesian approach to this kind of model. Advantages of the Bayesian approach are discussed, and results obtained from a simulation study are provided for illustration. The intended audience is statisticians/methodologists who either know about SEMs or simple Bayesian statistics, and Ph.D. students in statistics, psychometrics, or mathematical psychology.},
  doi       = {10.1016/j.jmp.2012.02.001},
  file      = {:D\:/Github/Informs/Articles/Tutorial_Bayesian_SEM.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{jones2018network,
  author    = {Jones, Payton J and Mair, Patrick and Riemann, Bradley C and Mugno, Beth L and McNally, Richard J},
  title     = {A network perspective on comorbid depression in adolescents with obsessive-compulsive disorder},
  journal   = {Journal of anxiety disorders},
  year      = {2018},
  volume    = {53},
  pages     = {1--8},
  file      = {:Articles/A Network Perspective on Comorbid Depression.pdf:PDF},
  publisher = {Elsevier},
}

@Article{Chao2017,
  author    = {Yi-Sheng Chao and Hau-tieng Wu and Marco Scutari and Tai-Shen Chen and Chao-Jung Wu and Madeleine Durand and Antoine Boivin},
  title     = {A network perspective on patient experiences and health status: the Medical Expenditure Panel Survey 2004 to 2011},
  journal   = {{BMC} Health Services Research},
  year      = {2017},
  volume    = {17},
  number    = {1},
  month     = {aug},
  doi       = {10.1186/s12913-017-2496-5},
  file      = {:Articles/10.1186.s12913-017-2496-5.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{McNally2017,
  author    = {R. J. McNally and P. Mair and B. L. Mugno and B. C. Riemann},
  title     = {Co-morbid obsessive{\textendash}compulsive disorder and depression: a Bayesian network approach},
  journal   = {Psychological Medicine},
  year      = {2017},
  volume    = {47},
  number    = {07},
  pages     = {1204--1214},
  month     = {jan},
  doi       = {10.1017/s0033291716003287},
  file      = {:Articles/McNally-Mair-Mugno-Riemann-in-press-Psychological-Medicine.pdf:PDF},
  publisher = {Cambridge University Press ({CUP})},
}

@Article{liu2018bus,
  author    = {Liu, Yi and Jia, Yuanhua and Feng, Xuesong and Wu, Jiang},
  title     = {Bus Route Design with a Bayesian Network Analysis of Bus Service Revenues},
  journal   = {Mathematical Problems in Engineering},
  year      = {2018},
  volume    = {2018},
  file      = {:Articles/2018. Bus Route.pdf:PDF},
  publisher = {Hindawi},
}

@Article{Deng2014,
  author    = {Li Deng},
  title     = {Deep Learning: Methods and Applications},
  journal   = {Foundations and Trends{\textregistered} in Signal Processing},
  year      = {2014},
  volume    = {7},
  number    = {3-4},
  pages     = {197--387},
  doi       = {10.1561/2000000039},
  file      = {:Articles/9781601988157-summary.pdf:PDF},
  publisher = {Now Publishers},
}

@Article{team2015stan,
  author  = {Team, Stan Developers},
  title   = {Stan Modeling Language: User’s Guide and Reference Manual},
  journal = {Version 2.12},
  year    = {2015},
}

@Article{Overstall2018,
  author      = {Antony M. Overstall and James M. McGree},
  title       = {Bayesian design of experiments for intractable likelihood models using coupled auxiliary models and multivariate emulation},
  abstract    = {A Bayesian design is given by maximising the expected utility over the design space. The utility is chosen to represent the aim of the experiment and its expectation is taken with respect to all unknowns: responses, parameters and/or models. Although straightforward in principle, there are several challenges to finding Bayesian designs in practice. Firstly, the expected utility is rarely available in closed form and requires approximation. Secondly, the expected utility needs to be maximised over a, potentially, high-dimensional design space. In the case of intractable likelihood models, these problems are compounded by the fact that the likelihood function, whose evaluation is required to approximate the expected utility, is not available in closed form. A strategy is proposed to find Bayesian designs for intractable likelihood models. It relies on the development of new methodology involving auxiliary modelling to approximate the expected utility, under an intractable likelihood model, applied with the latest approaches to maximising approximated expected utilities.},
  date        = {2018-03-19},
  eprint      = {1803.07018v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Articles/2018.arXiv.1803.07018.pdf:PDF},
  keywords    = {stat.ME},
}

@Article{Lakens2017,
  author  = {Lakens, D and Adolfi, FG and Albers, CJ and Anvari, F and Apps, MAJ and Argamon, SE and Zwaan, RA},
  title   = {Justify your alpha: A response to “Redefine statistical significance”},
  journal = {Retrieved from psyarxiv. com/9s3y6},
  year    = {2017},
  file    = {:D\:/Dropbox/Bayes_Reading/Justify Your Alpha.pdf:PDF},
}

@InProceedings{Dash2003,
  author       = {Dash, Denver and Cooper, Gregory F},
  title        = {Model-Averaging with Discrete Bayesian Network Classifiers},
  booktitle    = {Proc. 9th Int. Workshop Artif. Intell. Statist.},
  year         = {2003},
  pages        = {38--45},
  organization = {Citeseer},
  file         = {:C\:/Users/cjdua/Documents/download.pdf:PDF},
}

@Article{Amrhein2017,
  author    = {Valentin Amrhein and Sander Greenland},
  title     = {Remove, rather than redefine, statistical significance},
  journal   = {Nature Human Behaviour},
  year      = {2017},
  volume    = {2},
  number    = {1},
  pages     = {4--4},
  month     = {sep},
  doi       = {10.1038/s41562-017-0224-0},
  file      = {:Articles/2017.Nature_Human_Behaviour.pdf:PDF},
  publisher = {Springer Nature},
}

@Misc{,
  author = {{Stan Development Team}},
  title  = {shinystan: Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models.},
  year   = {2017},
  note   = {R package version 2.4.0},
  url    = {http://mc-stan.org/},
}

@Article{Williams2018,
  author    = {Daniel Williams},
  title     = {Predictive coding and thought},
  journal   = {Synthese},
  year      = {2018},
  month     = {mar},
  doi       = {10.1007/s11229-018-1768-x},
  file      = {:Articles/2018.Synthese.10.1007s11229-0181768-x.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Deaton2017,
  author    = {Angus Deaton and Nancy Cartwright},
  title     = {Understanding and misunderstanding randomized controlled trials},
  journal   = {Social Science {\&} Medicine},
  year      = {2017},
  month     = {dec},
  doi       = {10.1016/j.socscimed.2017.12.005},
  file      = {:Articles/1-s2.0-S0277953617307359-main.pdf:PDF},
  publisher = {Elsevier {BV}},
}

@Article{Li2018,
  author    = {Shaobo Li and Yongming Wu and Yanxia Xu and Jie Hu and Jianjun Hu},
  title     = {A Bayesian Network Based Adaptability Design of Product Structures for Function Evolution},
  journal   = {Applied Sciences},
  year      = {2018},
  volume    = {8},
  number    = {4},
  pages     = {493},
  month     = {mar},
  doi       = {10.3390/app8040493},
  file      = {:Articles/applsci-08-00493.pdf:PDF},
  publisher = {{MDPI} {AG}},
}

@Article{Chiappa2018,
  author      = {Silvia Chiappa and Thomas P. S. Gillam},
  title       = {Path-Specific Counterfactual Fairness},
  abstract    = {We consider the problem of learning fair decision systems in complex scenarios in which a sensitive attribute might affect the decision along both fair and unfair pathways. We introduce a causal approach to disregard effects along unfair pathways that simplifies and generalizes previous literature. Our method corrects observations adversely affected by the sensitive attribute, and uses these to form a decision. This avoids disregarding fair information, and does not require an often intractable computation of the path-specific effect. We leverage recent developments in deep learning and approximate inference to achieve a solution that is widely applicable to complex, non-linear scenarios.},
  date        = {2018-02-22},
  eprint      = {1802.08139v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1802.08139v1:PDF;:Articles/1802.08139.pdf:PDF},
  keywords    = {stat.ML},
}

@InProceedings{bareinboim2018,
  author    = {J. Zhang, E. Bareinboim},
  title     = {Fairness in Decision-Making -- The Causal Explanation Formula},
  booktitle = {Proceedings of the 32nd AAAI Conference on Artificial Intelligence},
  year      = {2018},
  file      = {:Articles/r30.pdf:PDF},
}

@Article{Chen238998,
  author    = {Gang Chen and Yaqiong Xiao and Paul A Taylor and Justin K Rajendra and Tracy Riggins and Fengji Geng and Elizabeth Redcay and Robert W Cox},
  title     = {Handling Multiplicity in Neuroimaging through Bayesian Lenses with Multilevel Modeling},
  journal   = {bioRxiv preprint},
  year      = {2017},
  month     = {dec},
  abstract  = {In neuroimaging, the multiplicity issue may sneak into data analysis through several channels, affecting
expected false positive rates (FPRs; type I errors) in diverse ways. One widely recognized aspect of multiplicity,
multiple testing, occurs when the investigator fits a separate model for each voxel in the brain. However,
multiplicity also occurs when the investigator conducts multiple comparisons within a model, tests two tails of
a t-test separately when prior information is unavailable about the directionality, and branches in the analytic
pipelines. The current practice of handling multiple testing through controlling the overall FPR in neuroimaging
under the null hypothesis significance testing (NHST) paradigm excessively penalizes the statistical power with
inflated type II errors. More fundamentally, the adoption of dichotomous decisions through sharp thresholding
under NHST may not be appropriate when the null hypothesis itself is not pragmatically relevant because the
effect of interest takes a continuum instead of discrete values and is not expected to be null in most brain regions.
When the noise inundates the signal, two different types of error are more relevant than the concept of FPR:
incorrect sign (type S) and incorrect magnitude (type M).
In light of these considerations, we introduce a different strategy using Bayesian hierarchical modeling (BHM)
to achieve two goals: 1) improving modeling efficiency via one integrative (instead of many separate) model and
dissolving the multiple testing issue, and 2) turning the focus of conventional NHST on FPR into quality control
by calibrating type S errors while maintaining a reasonable level of inference efficiency. The performance and
validity of this approach are demonstrated through an application at the region of interest (ROI) level, with all
the regions on an equal footing: unlike the current approaches under NHST, small regions are not disadvantaged
simply because of their physical size. In addition, compared to the massively univariate approach, BHM may
simultaneously achieve increased spatial specificity and inference efficiency. The benefits of BHM are illustrated in
model performance and quality checking using an experimental dataset. In addition, BHM offers an alternative,
confirmatory, or complementary approach to the conventional whole brain analysis under NHST, and promotes
results reporting in totality and transparency. The methodology also avoids sharp and arbitrary thresholding in
the p-value funnel to which the multidimensional data are reduced. The modeling approach with its auxiliary
tools will be available as part of the AFNI suite for general use.},
  doi       = {10.1101/238998},
  file      = {:Articles/238998.full.pdf:PDF},
  publisher = {Cold Spring Harbor Laboratory},
}

@Article{Romeijn2018,
  author    = {Jan-Willem Romeijn and Jon Williamson},
  title     = {Intervention and Identifiability in Latent Variable Modelling},
  journal   = {Minds and Machines},
  year      = {2018},
  month     = {mar},
  doi       = {10.1007/s11023-018-9460-y},
  file      = {:Articles/10.1007%2Fs11023-018-9460-y.pdf:PDF},
  publisher = {Springer Nature},
}

@Article{Hoffman2014,
  author  = {Hoffman, Matthew D and Gelman, Andrew},
  title   = {The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {1},
  pages   = {1593--1623},
}

@Article{Weber2016,
  author  = {Weber, Sebastian and Gelman, Andrew and Carpenter, Bob and Lee, Daniel and Betancourt, Michael and Vehtari, Aki and Racine, Amy},
  title   = {Hierarchical expectation propagation for Bayesian aggregation of average data},
  journal = {arXiv preprint arXiv:1602.02055},
  year    = {2016},
  file    = {:Articles/1602.02055.pdf:PDF},
}

@Article{Liu2016,
  author    = {G. Frank Liu and Baoguang Han and Xin Zhao and Qun Lin},
  title     = {A Comparison of Frequentist and Bayesian Model Based Approaches for~Missing Data Analysis: Case Study with a Schizophrenia Clinical Trial},
  journal   = {Statistics in Biopharmaceutical Research},
  year      = {2016},
  volume    = {8},
  number    = {1},
  pages     = {116--127},
  month     = {jan},
  abstract  = {Missing data are common in clinical trials and could lead to biased estimation of treatment effects. The National Research Council (NRC) report suggests that sensitivity analysis on missing data mechanism should be a mandatory component of the primary reporting of findings from clinical trials, and regulatory agencies are requesting more thorough sensitivity analyses from sponsors. However, recent literature research showed that missing data were almost always inadequately handled. This is partially due to the lack of standard software packages and straightforward implementation platform. With recent availability of flexible Bayesian software packages such as WinBUGS, SAS Proc MCMC, and Stan, it is relatively simple to develop Bayesian methods to address complex missing data problems while incorporating the uncertainty. In this article, we present a case study from the DIA Bayesian Scientific Working Group (BSWG) on Bayesian approaches for missing data analysis. We illustrate how to use Bayesian approaches to fit a few commonly used frequentist missing data models. The properties, advantage, and flexibility of the Bayesian analysis methods will be discussed using a case study based on a schizophrenia clinical trial. Supplementary materials for this article are available online.

Key Words: Bayesian, Longitudinal clinical trial, Missing data, Sensitivity analysis},
  doi       = {10.1080/19466315.2015.1077725},
  publisher = {Informa {UK} Limited},
}

@Article{Buerkner2016,
  author        = {B{\"u}rkner, Paul-Christian and others},
  title         = {brms: An R package for Bayesian multilevel models using Stan},
  journal       = {Journal of Statistical Software},
  year          = {2016},
  volume        = {80},
  number        = {1},
  pages         = {1--28},
  __markedentry = {[dulunche:]},
  abstract      = {The brms package implements Bayesian multilevel models in R using the probabilistic
programming language Stan. A wide range of distributions and link functions are
supported, allowing users to fit – among others – linear, robust linear, binomial, Poisson,
survival, response times, ordinal, quantile, zero-inflated, hurdle, and even non-linear
models all in a multilevel context. Further modeling options include autocorrelation of
the response variable, user defined covariance structures, censored data, as well as metaanalytic
standard errors. Prior specifications are flexible and explicitly encourage users
to apply prior distributions that actually reflect their beliefs. In addition, model fit can
easily be assessed and compared using posterior-predictive checks and leave-one-out crossvalidation.
If you use the software, please cite this article as published in the Journal of
Statistical Software Burkner ¨ (2017).
Keywords: Bayesian inference, multilevel model, ordinal data, MCMC, Stan, R.},
  file          = {:Articles/brms_overview.pdf:PDF},
}

@Article{Carpenter2016,
  author        = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matt and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Michael A and Guo, Jiqiang and Li, Peter and Riddell, Allen and others},
  title         = {Stan: A probabilistic programming language},
  journal       = {Journal of Statistical Software},
  year          = {2016},
  volume        = {20},
  number        = {2},
  pages         = {1--37},
  __markedentry = {[dulunche:6]},
  file          = {:Articles/Stan - Probabilistic Programming Language.pdf:PDF},
}

@Article{Gelman2015,
  author        = {Gelman, Andrew and Lee, Daniel and Guo, Jiqiang},
  title         = {Stan: A probabilistic programming language for Bayesian inference and optimization},
  journal       = {Journal of Educational and Behavioral Statistics},
  year          = {2015},
  volume        = {40},
  number        = {5},
  pages         = {530--543},
  __markedentry = {[dulunche:6]},
  publisher     = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{Lopez2018,
  author    = {Romain Lopez and Jeffrey Regier and Michael B Cole and Michael Jordan and Nir Yosef},
  title     = {Bayesian Inference for a Generative Model of Transcriptome Profiles from Single-cell {RNA} Sequencing},
  year      = {2018},
  month     = {mar},
  abstract  = {Transcriptome profiles of individual cells reflect true and often unexplored biological diversity, but are also affected by noise of biological and technical nature. This raises the need to explicitly model the resulting uncertainty and take it into account in any downstream analysis, such as dimensionality reduction, clustering, and differential expression. Here, we introduce Single-cell Variational Inference (scVI), a scalable framework for probabilistic representation and analysis of gene expression in single cells. Our model uses variational inference and stochastic optimization of deep neural networks to approximate the parameters that govern the distribution of expression values of each gene in every cell, using a non-linear mapping between the observations and a low-dimensional latent space. By doing so, scVI pools information between similar cells or genes while taking nuisance factors of variation such as batch effects and limited sensitivity into account. To evaluate scVI, we conducted a comprehensive comparative analysis to existing methods for distributional modeling and dimensionality reduction, all of which rely on generalized linear models. We first show that scVI scales to over one million cells, whereas competing algorithms can process at most tens of thousands of cells. Next, we show that scVI fits unseen data more closely and can impute missing data more accurately, both indicative of a better generalization capacity. We then utilize scVI to conduct a set of fundamental analysis tasks -- including batch correction, visualization, clustering and differential expression -- and demonstrate its accuracy in comparison to the state-of-the-art tools in each task. scVI is publicly available, and can be readily used as a principled and inclusive solution for multiple tasks of single-cell RNA sequencing data analysis.},
  doi       = {10.1101/292037},
  file      = {:Articles/292037.full.pdf:PDF},
  publisher = {Cold Spring Harbor Laboratory},
}

@Comment{jabref-meta: databaseType:bibtex;}
